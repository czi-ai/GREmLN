{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from os.path import join, abspath, dirname\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from scGraphLLM.data import *\n",
    "from scGraphLLM.GNN_modules import *\n",
    "from scGraphLLM.MLP_modules import *\n",
    "from scGraphLLM._globals import *\n",
    "from scGraphLLM.flash_transformer import GDTransformer\n",
    "from scGraphLLM.config import *\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_to_node = pd.read_csv(\"/hpc/projects/group.califano/GLM/data/cellxgene_gene2index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_to_node = gene_to_node.set_index(\"gene_name\")[\"idx\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneEmbeddingDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        assert os.path.exists(path), f\"File not found: {path}\"\n",
    "        self.path = path\n",
    "        self.embedding = np.load(self.path, allow_pickle=True)\n",
    "        self.x = self.embedding[\"x\"]\n",
    "        self.pad_indices = self.embedding[\"pad_indices\"]\n",
    "        self.edges = self.embedding[\"edges\"].item()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedding[\"x\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        return {\n",
    "            \"x\": torch.tensor(self.x[idx]), \n",
    "            \"pad_indices\": torch.tensor(self.pad_indices[idx]),\n",
    "            \"edges\": torch.tensor(self.edges[idx])\n",
    "        }\n",
    "\n",
    "def embedding_collate_fn(batch):\n",
    "    return {\n",
    "        \"x\": torch.stack([item[\"x\"] for item in batch]),\n",
    "        \"pad_indices\": torch.stack([item[\"pad_indices\"] for item in batch]),\n",
    "        \"edges\": [item[\"edges\"] for item in batch]\n",
    "    }\n",
    "    \n",
    "scgpt_embedding_dataset = GeneEmbeddingDataset(\n",
    "    path=\"/hpc/mydata/rowan.cassius/data/scGPT/human_immune/cell_type/cd8_t_cells/embeddings/scgpt/embedding.npz\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "scgpt_dataloader = DataLoader(\n",
    "    scgpt_embedding_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False,\n",
    "    collate_fn=embedding_collate_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_predictor_scgpt = LinkPredictHead(512, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = scgpt_embedding_dataset[0][\"x\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1048, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_i, x_j = x[0,:].to(device), x[1,:].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4882],\n",
       "        [0.5073],\n",
       "        [0.4822],\n",
       "        ...,\n",
       "        [0.5088],\n",
       "        [0.5088],\n",
       "        [0.5088]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link_predictor_scgpt(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_pred_loss(predictor, node_embedding, pad_indices, edge_index_list):\n",
    "    \"\"\"\n",
    "    predictor (nn.Module), predictor module\n",
    "    node_embedding: node embedding matrix\n",
    "    mask_locs: mask of valid nodes\n",
    "    edge_index_list:\n",
    "    \"\"\"\n",
    "    pos_out = []\n",
    "    neg_out = []\n",
    "    pos_labels = []\n",
    "    neg_labels = []\n",
    "\n",
    "    batch_size, num_nodes, embed_dim = node_embedding.shape\n",
    "    device = node_embedding.device\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # masked_nodes = torch.where(mask_locs[batch])[0]\n",
    "        # if masked_nodes.numel() == 0:\n",
    "        #     continue\n",
    "        # masked_nodes = masked_nodes.to(device)\n",
    "        # edge_index = edge_index_list[batch].to(device)\n",
    "        # masked_nodes_bool = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "        # masked_nodes_bool[masked_nodes] = True\n",
    "\n",
    "        # src_nodes = edge_index[0]\n",
    "        # dst_nodes = edge_index[1]\n",
    "        # edge_mask = masked_nodes_bool[src_nodes] & masked_nodes_bool[dst_nodes]\n",
    "        # pos_edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "        pos_edge_index = edge_index_list[i].to(device)\n",
    "        num_nodes = pad_indices[i]\n",
    "\n",
    "        if pos_edge_index.size(1) == 0:\n",
    "            continue\n",
    "\n",
    "        num_neg_samples = pos_edge_index.size(1)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            # edge_index=edge_index,\n",
    "            edge_index=pos_edge_index,\n",
    "            num_nodes=num_nodes,\n",
    "            num_neg_samples=num_neg_samples,\n",
    "            method=\"sparse\"\n",
    "        ).to(device)\n",
    "\n",
    "        # Positive scores\n",
    "        src_emb_pos = node_embedding[i, pos_edge_index[0]]\n",
    "        dst_emb_pos = node_embedding[i, pos_edge_index[1]]\n",
    "        pos_scores = predictor(src_emb_pos, dst_emb_pos)\n",
    "        pos_out.append(pos_scores)\n",
    "        pos_labels.append(torch.ones_like(pos_scores, device=device))  # Positive labels (1)\n",
    "\n",
    "        # Negative scores\n",
    "        src_emb_neg = node_embedding[i, neg_edge_index[0]]\n",
    "        dst_emb_neg = node_embedding[i, neg_edge_index[1]]\n",
    "        neg_scores = predictor(src_emb_neg, dst_emb_neg)\n",
    "        neg_out.append(neg_scores)\n",
    "        neg_labels.append(torch.zeros_like(neg_scores, device=device))  # Negative labels (0)\n",
    "\n",
    "    if pos_out:\n",
    "        pos_out = torch.cat(pos_out, dim=0)\n",
    "        neg_out = torch.cat(neg_out, dim=0)\n",
    "        pos_labels = torch.cat(pos_labels, dim=0)\n",
    "        neg_labels = torch.cat(neg_labels, dim=0)\n",
    "\n",
    "        # Loss calculation\n",
    "        pos_loss = -torch.log(pos_out + 1e-10).mean()\n",
    "        neg_loss = -torch.log(1 - neg_out + 1e-10).mean()\n",
    "\n",
    "        # Concatenate outputs and labels\n",
    "        all_outputs = torch.cat([pos_out, neg_out], dim=0)\n",
    "        all_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        return pos_loss + neg_loss, all_outputs, all_labels\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=device), torch.tensor([], device=device), torch.tensor([], device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape: torch.Size([10, 1048, 512])\n",
      "pad_indices.shape: torch.Size([10])\n",
      "len(edges): 10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlen(edges): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124medges\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# calculate link prediciton loss for batch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mlink_pred_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlink_predictor_scgpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_embedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpad_indices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43medges\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 37\u001b[0m, in \u001b[0;36mlink_pred_loss\u001b[0;34m(predictor, node_embedding, pad_indices, edge_index_list)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     36\u001b[0m num_neg_samples \u001b[38;5;241m=\u001b[39m pos_edge_index\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m neg_edge_index \u001b[38;5;241m=\u001b[39m \u001b[43mnegative_sampling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# edge_index=edge_index,\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_edge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_neg_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_neg_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msparse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Positive scores\u001b[39;00m\n\u001b[1;32m     46\u001b[0m src_emb_pos \u001b[38;5;241m=\u001b[39m node_embedding[i, pos_edge_index[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m/hpc/mydata/rowan.cassius/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/torch_geometric/utils/_negative_sampling.py:69\u001b[0m, in \u001b[0;36mnegative_sampling\u001b[0;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[1;32m     66\u001b[0m     bipartite \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     force_undirected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m idx, population \u001b[38;5;241m=\u001b[39m \u001b[43medge_index_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbipartite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                       \u001b[49m\u001b[43mforce_undirected\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m population:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edge_index\u001b[38;5;241m.\u001b[39mnew_empty((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/hpc/mydata/rowan.cassius/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/torch_geometric/utils/_negative_sampling.py:327\u001b[0m, in \u001b[0;36medge_index_to_vector\u001b[0;34m(edge_index, size, bipartite, force_undirected)\u001b[0m\n\u001b[1;32m    324\u001b[0m row, col \u001b[38;5;241m=\u001b[39m edge_index\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bipartite:  \u001b[38;5;66;03m# No need to account for self-loops.\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (row \u001b[38;5;241m*\u001b[39m \u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(col)\n\u001b[1;32m    328\u001b[0m     population \u001b[38;5;241m=\u001b[39m size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m size[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m idx, population\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "for batch in scgpt_dataloader:\n",
    "    print(f\"x.shape: {batch['x'].shape}\")\n",
    "    print(f\"pad_indices.shape: {batch['pad_indices'].shape}\")\n",
    "    print(f\"len(edges): {len(batch['edges'])}\")\n",
    "    # calculate link prediciton loss for batch\n",
    "    loss, _, _ = link_pred_loss(\n",
    "        predictor=link_predictor_scgpt,\n",
    "        node_embedding=batch[\"x\"], \n",
    "        pad_indices=batch[\"pad_indices\"],\n",
    "        edge_index_list=batch[\"edges\"]\n",
    "    )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.califano/GLM/data/cxg_cache_4096/train\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hpc/projects/group.califano/GLM/data/cxg_cache_4096/valSG\n",
      "/hpc/projects/group.califano/GLM/data/cxg_cache_4096/valHOG\n"
     ]
    }
   ],
   "source": [
    "transformer_data_module = GraphTransformerDataModule(\n",
    "    graph_kernel_attn_4096.data_config, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "train_transformer_dl = transformer_data_module.train_dataloader()\n",
    "val_transformer_dl = transformer_data_module.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_hog_data = val_transformer_dl[-1]\n",
    "val_sg_data = val_transformer_dl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_pred_loss(predictor, node_embedding, mask_locs, edge_index_list):\n",
    "    pos_out = []\n",
    "    neg_out = []\n",
    "    pos_labels = []\n",
    "    neg_labels = []\n",
    "\n",
    "    batch_size, num_nodes, embed_dim = node_embedding.shape\n",
    "    device = node_embedding.device\n",
    "\n",
    "    for batch in range(batch_size):\n",
    "        masked_nodes = torch.where(mask_locs[batch])[0]\n",
    "        if masked_nodes.numel() == 0:\n",
    "            continue\n",
    "        masked_nodes = masked_nodes.to(device)\n",
    "        edge_index = edge_index_list[batch].to(device)\n",
    "        masked_nodes_bool = torch.zeros(num_nodes, dtype=torch.bool, device=device)\n",
    "        masked_nodes_bool[masked_nodes] = True\n",
    "        src_nodes = edge_index[0]\n",
    "        dst_nodes = edge_index[1]\n",
    "        edge_mask = masked_nodes_bool[src_nodes] & masked_nodes_bool[dst_nodes]\n",
    "        pos_edge_index = edge_index[:, edge_mask]\n",
    "        if pos_edge_index.size(1) == 0:\n",
    "            continue\n",
    "\n",
    "        num_neg_samples = pos_edge_index.size(1)\n",
    "        neg_edge_index = negative_sampling(\n",
    "            edge_index=edge_index,\n",
    "            num_nodes=num_nodes,\n",
    "            num_neg_samples=num_neg_samples,\n",
    "            method='sparse'\n",
    "        ).to(device)\n",
    "\n",
    "        # Positive scores\n",
    "        src_emb_pos = node_embedding[batch, pos_edge_index[0]]\n",
    "        dst_emb_pos = node_embedding[batch, pos_edge_index[1]]\n",
    "        pos_scores = predictor(src_emb_pos, dst_emb_pos)\n",
    "        pos_out.append(pos_scores)\n",
    "        pos_labels.append(torch.ones_like(pos_scores, device=device))  # Positive labels (1)\n",
    "\n",
    "        # Negative scores\n",
    "        src_emb_neg = node_embedding[batch, neg_edge_index[0]]\n",
    "        dst_emb_neg = node_embedding[batch, neg_edge_index[1]]\n",
    "        neg_scores = predictor(src_emb_neg, dst_emb_neg)\n",
    "        neg_out.append(neg_scores)\n",
    "        neg_labels.append(torch.zeros_like(neg_scores, device=device))  # Negative labels (0)\n",
    "\n",
    "    if pos_out:\n",
    "        pos_out = torch.cat(pos_out, dim=0)\n",
    "        neg_out = torch.cat(neg_out, dim=0)\n",
    "        pos_labels = torch.cat(pos_labels, dim=0)\n",
    "        neg_labels = torch.cat(neg_labels, dim=0)\n",
    "\n",
    "        # Loss calculation\n",
    "        pos_loss = -torch.log(pos_out + 1e-10).mean()\n",
    "        neg_loss = -torch.log(1 - neg_out + 1e-10).mean()\n",
    "\n",
    "        # Concatenate outputs and labels\n",
    "        all_outputs = torch.cat([pos_out, neg_out], dim=0)\n",
    "        all_labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        return pos_loss + neg_loss, all_outputs, all_labels\n",
    "    else:\n",
    "        return torch.tensor(0.0, device=device), torch.tensor([], device=device), torch.tensor([], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_gpu(data):\n",
    "    if isinstance(data, torch.Tensor):\n",
    "        return data.to('cuda')  # Send tensor to GPU\n",
    "    elif isinstance(data, list):\n",
    "        return [send_to_gpu(item) for item in data]  # Recursively process lists\n",
    "    elif isinstance(data, dict):\n",
    "        return {key: send_to_gpu(value) for key, value in data.items()}  # Recursively process dicts\n",
    "    else:\n",
    "        return data  # If not a tensor or list/dict, leave unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_train_step(pretrained_model, ft_model, batch, opt):\n",
    "    pretrained_model.eval()\n",
    "    ft_model.train()\n",
    "    batch = send_to_gpu(batch)\n",
    "    embedding, target_gene_ids, target_rank_ids, mask_locs, edge_index_list, num_nodes_list = pretrained_model(batch)\n",
    "    L_g, _, _ = link_pred_loss(ft_model, embedding, mask_locs[0], edge_index_list)\n",
    "    L_g.backward()\n",
    "    opt.step()\n",
    "    return L_g\n",
    "\n",
    "def fine_tune(train_dataloader, pretrained_model, ft_model, lr=1e-3, num_epochs=100, max_num_batches=200):\n",
    "    train_losses = []\n",
    "    opt = torch.optim.Adam(ft_model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss_epoch = 0\n",
    "        train_batches = len(train_dataloader)\n",
    "        num_batches = 0\n",
    "        for batch in  tqdm.tqdm(train_dataloader, desc=\"Training\", leave=False):\n",
    "            train_loss = fine_tune_train_step(pretrained_model, ft_model, batch, opt)\n",
    "            train_loss_epoch += train_loss.item()\n",
    "            num_batches += 1\n",
    "            if num_batches >= max_num_batches:\n",
    "                break\n",
    "        train_loss_epoch /= train_batches\n",
    "        train_losses.append(train_loss_epoch)\n",
    "        print(f\"Train loss: {train_loss_epoch:.4f}\")\n",
    "    return train_losses\n",
    "\n",
    "def predict_and_compare(test_dataloader, pretrain1, pretrain2, model1, model2, max_num_batches=100):\n",
    "    model1.eval().to(\"cuda\")\n",
    "    model2.eval().to(\"cuda\")\n",
    "    pretrain1.eval().to(\"cuda\")\n",
    "    pretrain2.eval().to(\"cuda\")\n",
    "    \n",
    "    all_preds1 = []\n",
    "    all_preds2 = []\n",
    "    all_labels = []\n",
    "    n_b = 0\n",
    "    for batch in tqdm.tqdm(test_dataloader, leave=False):\n",
    "        batch = send_to_gpu(batch)\n",
    "        embedding1, target_gene_ids, target_rank_ids, mask_locs, edge_index_list, num_nodes_list = pretrain1(batch)\n",
    "        embedding2, target_gene_ids, target_rank_ids, mask_locs, edge_index_list, num_nodes_list = pretrain2(batch)\n",
    "        \n",
    "        L_g1, preds1, labels = link_pred_loss(model1, embedding1, mask_locs[0], edge_index_list)\n",
    "        L_g2, preds2, labels = link_pred_loss(model2, embedding2, mask_locs[0], edge_index_list)\n",
    "        \n",
    "        all_preds1.extend(preds1.cpu().detach().numpy())\n",
    "        all_preds2.extend(preds2.cpu().detach().numpy())\n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        \n",
    "        n_b += 1\n",
    "        if n_b >= max_num_batches:\n",
    "            break\n",
    "    \n",
    "    # AUROC\n",
    "    fpr1, tpr1, _ = roc_curve(all_labels, all_preds1)\n",
    "    fpr2, tpr2, _ = roc_curve(all_labels, all_preds2)\n",
    "    auc1 = auc(fpr1, tpr1)\n",
    "    auc2 = auc(fpr2, tpr2)\n",
    "    \n",
    "    # PR\n",
    "    p1, r1, _ = precision_recall_curve(all_labels, all_preds1)\n",
    "    p2, r2, _ = precision_recall_curve(all_labels, all_preds2)\n",
    "    apr1 = average_precision_score(all_labels, all_preds1)\n",
    "    apr2 = average_precision_score(all_labels, all_preds2)\n",
    "    \n",
    "    return fpr1, tpr1, auc1, fpr2, tpr2, auc2, p1, r1, apr1, p2, r2, apr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auroc_curves(fpr1, tpr1, auc1, fpr2, tpr2, auc2):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr1, tpr1, label=f\"fine-tune w. vanilla embedding (AUC = {auc1:.3f})\")\n",
    "    plt.plot(fpr2, tpr2, label=f\"fine-tune w. GraphDKA embedding (AUC = {auc2:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Fine tuning AUROC, link pred\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_curves(precision1, recall1, ap1, precision2, recall2, ap2):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot the first PR curve\n",
    "    plt.plot(recall1, precision1, label=f\"fine-tune w. vanilla embedding (Avg. Prec. = {ap1:.3f})\", linestyle='-', marker=None)\n",
    "\n",
    "    # Plot the second PR curve\n",
    "    plt.plot(recall2, precision2, label=f\"fine-tune w. GraphDKA embedding (Avg. Prec. = {ap2:.3f})\", linestyle='--', marker=None)\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Fine tuning Precision-Recall Curve, link pred\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "\n",
    "    # Set axis limits for better visualization\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_batches(dataloader, n):\n",
    "    # Convert the dataloader to a list of batches\n",
    "    batches = list(dataloader)\n",
    "    # Randomly sample n batches\n",
    "    sampled_batches = random.sample(batches, n)\n",
    "    return sampled_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_lp = LinkPredictHead(256, 1).to(\"cuda\")\n",
    "gdk_lp = LinkPredictHead(256, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_predictor_geneformer = LinkPredictHead(256, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_predictor_scf = LinkPredictHead(512, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_predictor_scgpt = LinkPredictHead(512, 1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vanilla_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vanilla_loss \u001b[38;5;241m=\u001b[39m fine_tune(val_sg_data, pretrained_model\u001b[38;5;241m=\u001b[39m\u001b[43mvanilla_model\u001b[49m, ft_model\u001b[38;5;241m=\u001b[39mvanilla_lp, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, max_num_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vanilla_model' is not defined"
     ]
    }
   ],
   "source": [
    "vanilla_loss = fine_tune(val_sg_data, pretrained_model=vanilla_model, ft_model=vanilla_lp, num_epochs=1, max_num_batches=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
