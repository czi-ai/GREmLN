/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/fabric/connector.py:571: `precision=bf16` is supported for historical reasons but its usage is discouraged. Please set your precision to bf16-mixed instead!
Using bfloat16 Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 8 processes
----------------------------------------------------------------------------------------------------

/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]

  | Name                 | Type          | Params | Mode
---------------------------------------------------------------
0 | gene_embedding       | Embedding     | 4.9 M  | train
1 | rank_embedding       | Embedding     | 26.1 K | train
2 | rank_prediction_head | RobertaLMHead | 316 K  | train
3 | transformer_encoder  | ModuleList    | 18.9 M | train
---------------------------------------------------------------
24.2 M    Trainable params
0         Non-trainable params
24.2 M    Total params
96.820    Total estimated model params size (MB)
153       Modules in train mode
0         Modules in eval mode
SLURM auto-requeueing enabled. Setting signal handlers.
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
Epoch 9: 100%|██████████| 1/1 [00:02<00:00,  0.42it/s, v_num=4px7]
copying checkpoints to shared dir                                     
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('valSG_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('valSG_mlm_rankonly_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('valSG_rank_perplexity', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('valHOG_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('valHOG_mlm_rankonly_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/hpc/mydata/leo.dupire/anaconda/23.1.0-3/x86_64/envs/scllm/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('valHOG_rank_perplexity', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
`Trainer.fit` stopped: `max_epochs=10` reached.
Run Completed Succsessfully.
