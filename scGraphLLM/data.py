import torch
from torch_geometric.data import Data, InMemoryDataset, DataLoader, Batch
from torch_geometric.loader import NeighborLoader
from torch_geometric.nn import MessagePassing
import numpy as np
import pandas as pd 
from _globals import ZERO_IDX

def aracne_to_edge_list(aracne_out, gene_to_node_file):
    network = pd.read_csv(aracne_out +"/aracne/consolidated-net_defaultid.tsv", sep = "\t")
    network_genes = list(set(network["regulator.values"].to_list() + network["target.values"].to_list()))
    global_gene_to_node_index = {row.gene_name:row.idx for _,row in pd.read_csv(gene_to_node_file).iterrows()}
    ranks = pd.read_csv(aracne_out + "/rank_raw.csv", index_col=0)[network_genes]
    is_nonempty = ~((ranks == ZERO_IDX).sum(axis = 1) == ranks.shape[1]-2) ## must have atleast 3 genes
    ranks = ranks[is_nonempty]

    local_gene_to_node_index = {gene:i for i, gene in enumerate(network_genes)}
    edges = network[['regulator.values', 'target.values', 'mi.values']]
    edges['regulator.values'] = edges['regulator.values'].map(local_gene_to_node_index)
    edges['target.values'] = edges['target.values'].map(local_gene_to_node_index)
    edge_list = torch.tensor(np.array(edges[['regulator.values', 'target.values']])).T
    edge_weights = torch.tensor(np.array(edges['mi.values']))
    node_indices = torch.tensor(np.array([global_gene_to_node_index[gene] for gene in network_genes]))

    return node_indices, edge_list, edge_weights, ranks.values

def node_batching(node_embedding, ranks, network, genes, batch_size=64, 
                 neigborhood_size=-1, num_hops=1):
    node_indices = edge_list, edge_weights = aracne_to_edge_list(network=network, genes=genes)
    combined_data = CombinationDataset(edge_index=edge_list, edge_weight=edge_weights, 
                                       node_embedding=node_embedding, rank_embedding=ranks)
    
    dataloader = NeighborLoader(data=combined_data, replace=False, num_neighbors=[neigborhood_size] * num_hops, 
                                input_nodes=None, subgraph_type="bidirectional", disjoint=False,
                                weight_attr="edge_weight", batch_size=batch_size, shuffle=True
                                )
    
    return dataloader, combined_data


# Customized combination dataset that returns node embedding and rank embedding
## node index corresponds to a global gene<>index mapping which is used to get the initial gene embedding - 
## this is why we cannot usethe batch.n_id generated by the NeighborLoader for getting the node index 
class CombinationDataset(Data):
    def __init__(self, edge_index, edge_weight, node_index, rank_embedding):
        super(CombinationDataset, self).__init__()
        self.edge_index = edge_index
        self.edge_weight = edge_weight
        self.x = node_index
        self.rank_embedding = rank_embedding



def repack_ranks(ranks):
    ### right now the ranks are ordered by gene, but we want to order them by rank
    enough_genes = ~((ranks == ZERO_IDX).sum(axis = 1) == ranks.shape[1]-2)
    ranks = ranks[enough_genes, :]
    repacked_ranks = []
    repacked_indices = []
    for i in range(ranks.shape[0]):
        row  = [(ranks[i,j],j) for j in range(ranks.shape[1])]
        row  = sorted(row, key=lambda x: x[0])
        rank_values, rank_indices = zip(*row)
        rank_values = np.array(rank_values)
        rank_indices = np.array(rank_indices)
        nonzero = rank_values != ZERO_IDX
        repacked_ranks.append(torch.tensor(rank_values[nonzero], dtype = torch.long) )
        repacked_indices.append(torch.tensor(rank_indices[nonzero], dtype = torch.long) )
    return torch.nested.nested_tensor(repacked_ranks), torch.nested.nested_tensor(repacked_indices)

def pad_make_masks(tns_list):
    lens = [t.shape[0] for t in tns_list]
    max_len = max(lens)
    mask = torch.zeros(len(tns_list), max_len)
    for i, l in enumerate(lens):
        mask[i, :l] = 1
    padded = torch.nn.utils.rnn.pad_sequence(tns_list, batch_first=True)
    return padded, mask.to(padded.device)

## this wraps the whole data batching process for multiple graphs into a single dataset, so that data elements 
## dont need to stored outsdie of data loading code - main to deal with accessing ranks from the
## right now loads everything into mem, but can easily be modified to first write to then read from disk 
## only issue is that the node batching itself is fixed - meaning that the same nodes are used for each batch
class MultiGraphWrapperDataset:
    def __init__(self, aracne_outdirs, gene_to_node_file, batch_size=64, 
                 neigborhood_size=-1, num_hops=1, shuffle_nl=True):
        self.batched_data = []
        for outdir in aracne_outdirs:
            node_indices, edge_list, edge_weights, ranks = aracne_to_edge_list(outdir, gene_to_node_file)

            dataset = CombinationDataset(edge_index=edge_list, edge_weight=edge_weights, 
                                        node_index=node_indices, rank_embedding=ranks)
            nl= NeighborLoader(data=dataset, replace=False, num_neighbors=[neigborhood_size] * num_hops, 
                                    input_nodes=None, subgraph_type="bidirectional", disjoint=False,
                                    weight_attr="edge_weight", batch_size=batch_size, shuffle=shuffle_nl)
            self.batched_data+= [(batch.x, batch.edge_index, batch.edge_weight, repack_ranks(dataset.rank_embedding[:,batch.n_id]) ) for batch in nl]
    def __len__(self):
        return len(self.batched_data)
    def __getitem__(self, idx):
        return self.batched_data[idx]


